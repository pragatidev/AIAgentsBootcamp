{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ LLM Provider Setup Guide - Choose Your AI Engine\n",
        "\n",
        "## Learn to Setup and Switch Between Popular LLM Providers\n",
        "\n",
        "This notebook shows you how to configure and test different LLM providers so you can easily switch between them in any course project.\n",
        "\n",
        "### üéØ What You'll Learn:\n",
        "- Setup and test **Ollama** (FREE local)\n",
        "- Setup and test **DeepSeek** (ultra-cheap: $0.14/1M)\n",
        "- Setup and test **Groq** (ultra-fast: $0.05/1M)\n",
        "- Setup and test **Google Gemini** (multimodal: $0.075/1M)\n",
        "- Setup and test **Anthropic Claude** (premium: $3/1M)\n",
        "- Setup and test **OpenAI GPT** (standard: $2.5/1M)\n",
        "\n",
        "### üí° Goal:\n",
        "By the end, you'll have reusable code templates to use any provider in future notebooks!\n",
        "\n",
        "**Prerequisites:** API keys already in your .env file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Cell 1: Install Dependencies & Load Environment\n",
        "\n",
        "First, let's install all the packages we need and load our environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All dependencies installed!\n",
            "‚úÖ Environment variables loaded!\n",
            "üéØ Ready to test LLM providers!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "import time\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")\n",
        "print(\"‚úÖ Environment variables loaded!\")\n",
        "print(\"üéØ Ready to test LLM providers!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üÜì Cell 2: Ollama (FREE Local AI)\n",
        "\n",
        "Test local Ollama setup - completely FREE with unlimited usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üÜì TESTING OLLAMA (FREE LOCAL AI)\n",
            "========================================\n",
            "‚úÖ Ollama server is running\n",
            "‚è±Ô∏è Response time: 7.51 seconds\n",
            "ü§ñ Response: I'm an artificial intelligence designed to provide helpful and informative responses, and I'd like to greet you with a virtual smile!\n",
            "üí∞ Cost: FREE (unlimited usage)\n",
            "\n",
            "üìã Copy-paste template for projects:\n",
            "```python\n",
            "from langchain_ollama import OllamaLLM\n",
            "llm = OllamaLLM(model='llama3.2', temperature=0.3)\n",
            "response = llm.invoke('Your prompt here')\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Test Ollama Local AI\n",
        "print(\"üÜì TESTING OLLAMA (FREE LOCAL AI)\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Check if Ollama is running\n",
        "    response = requests.get(\"http://localhost:11434/api/version\", timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        print(\"‚úÖ Ollama server is running\")\n",
        "        \n",
        "        # Test with LangChain\n",
        "        from langchain_ollama import OllamaLLM\n",
        "        \n",
        "        llm = OllamaLLM(model=\"llama3.2\", temperature=0.3)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        test_response = llm.invoke(\"Say hello and introduce yourself in one sentence.\")\n",
        "        response_time = time.time() - start_time\n",
        "        \n",
        "        print(f\"‚è±Ô∏è Response time: {response_time:.2f} seconds\")\n",
        "        print(f\"ü§ñ Response: {test_response}\")\n",
        "        print(\"üí∞ Cost: FREE (unlimited usage)\")\n",
        "        print(\"\\nüìã Copy-paste template for projects:\")\n",
        "        print(\"```python\")\n",
        "        print(\"from langchain_ollama import OllamaLLM\")\n",
        "        print(\"llm = OllamaLLM(model='llama3.2', temperature=0.3)\")\n",
        "        print(\"response = llm.invoke('Your prompt here')\")\n",
        "        print(\"```\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå Ollama server not responding\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Ollama setup issue: {str(e)}\")\n",
        "    print(\"üí° Install Ollama: https://ollama.ai/\")\n",
        "    print(\"üí° Download model: ollama pull llama3.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí∞ Cell 3: DeepSeek (Ultra-Cheap: $0.14/1M tokens)\n",
        "\n",
        "Test DeepSeek API - excellent value for coding and general tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí∞ TESTING DEEPSEEK (ULTRA-CHEAP)\n",
            "===================================\n",
            "‚ùå DeepSeek error: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n"
          ]
        }
      ],
      "source": [
        "# Test DeepSeek API\n",
        "print(\"üí∞ TESTING DEEPSEEK (ULTRA-CHEAP)\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "deepseek_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
        "\n",
        "if deepseek_key and deepseek_key != \"your_deepseek_api_key_here\":\n",
        "    try:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        \n",
        "        llm = ChatOpenAI(\n",
        "            api_key=deepseek_key,\n",
        "            base_url=\"https://api.deepseek.com\",\n",
        "            model=\"deepseek-chat\",\n",
        "            temperature=0.3\n",
        "        )\n",
        "        \n",
        "        start_time = time.time()\n",
        "        test_response = llm.invoke(\"Write a simple Python function to add two numbers.\")\n",
        "        response_time = time.time() - start_time\n",
        "        \n",
        "        print(\"‚úÖ DeepSeek API working!\")\n",
        "        print(f\"‚è±Ô∏è Response time: {response_time:.2f} seconds\")\n",
        "        print(f\"ü§ñ Response: {test_response.content}\")\n",
        "        print(\"üí∞ Cost: $0.14 per 1M tokens\")\n",
        "        \n",
        "        print(\"\\nüìã Copy-paste template for projects:\")\n",
        "        print(\"```python\")\n",
        "        print(\"from langchain_openai import ChatOpenAI\")\n",
        "        print(\"llm = ChatOpenAI(\")\n",
        "        print(\"    api_key=os.getenv('DEEPSEEK_API_KEY'),\")\n",
        "        print(\"    base_url='https://api.deepseek.com',\")\n",
        "        print(\"    model='deepseek-chat',\")\n",
        "        print(\"    temperature=0.3\")\n",
        "        print(\")\")\n",
        "        print(\"response = llm.invoke('Your prompt here')\")\n",
        "        print(\"```\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå DeepSeek error: {str(e)}\")\n",
        "else:\n",
        "    print(\"‚ùå DeepSeek API key not found in .env file\")\n",
        "    print(\"üîë Get your key: https://platform.deepseek.com/\")\n",
        "    print(\"üìù Add to .env: DEEPSEEK_API_KEY=your_key_here\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Cell 4: Groq (Ultra-Fast: $0.05/1M tokens)\n",
        "\n",
        "Test Groq API - sub-100ms responses with LPU technology."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö° TESTING GROQ (ULTRA-FAST)\n",
            "==============================\n",
            "‚úÖ Groq API working!\n",
            "‚è±Ô∏è Response time: 0.27 seconds (ultra-fast!)\n",
            "ü§ñ Response: An AI agent is a software program that perceives its environment, makes decisions, and takes actions to achieve specific goals or objectives, often using machine learning and artificial intelligence techniques.\n",
            "üí∞ Cost: $0.05 per 1M tokens\n",
            "\n",
            "üìã Copy-paste template for projects:\n",
            "```python\n",
            "from groq import Groq\n",
            "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
            "response = client.chat.completions.create(\n",
            "    messages=[{'role': 'user', 'content': 'Your prompt here'}],\n",
            "    model='llama-3.1-8b-instant',\n",
            "    temperature=0.3\n",
            ")\n",
            "result = response.choices[0].message.content\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Test Groq API\n",
        "print(\"‚ö° TESTING GROQ (ULTRA-FAST)\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "if groq_key and groq_key != \"your_groq_api_key_here\":\n",
        "    try:\n",
        "        from groq import Groq\n",
        "        \n",
        "        client = Groq(api_key=groq_key)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Explain AI agents in one sentence.\"}],\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            temperature=0.3\n",
        "        )\n",
        "        response_time = time.time() - start_time\n",
        "        \n",
        "        print(\"‚úÖ Groq API working!\")\n",
        "        print(f\"‚è±Ô∏è Response time: {response_time:.2f} seconds (ultra-fast!)\")\n",
        "        print(f\"ü§ñ Response: {response.choices[0].message.content}\")\n",
        "        print(\"üí∞ Cost: $0.05 per 1M tokens\")\n",
        "        \n",
        "        print(\"\\nüìã Copy-paste template for projects:\")\n",
        "        print(\"```python\")\n",
        "        print(\"from groq import Groq\")\n",
        "        print(\"client = Groq(api_key=os.getenv('GROQ_API_KEY'))\")\n",
        "        print(\"response = client.chat.completions.create(\")\n",
        "        print(\"    messages=[{'role': 'user', 'content': 'Your prompt here'}],\")\n",
        "        print(\"    model='llama-3.1-8b-instant',\")\n",
        "        print(\"    temperature=0.3\")\n",
        "        print(\")\")\n",
        "        print(\"result = response.choices[0].message.content\")\n",
        "        print(\"```\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Groq error: {str(e)}\")\n",
        "else:\n",
        "    print(\"‚ùå Groq API key not found in .env file\")\n",
        "    print(\"üîë Get your key: https://console.groq.com/\")\n",
        "    print(\"üìù Add to .env: GROQ_API_KEY=your_key_here\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Cell 5: Google Gemini (Multimodal: $0.075/1M tokens)\n",
        "\n",
        "Test Google Gemini API - handles text, images, video, and audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üé® TESTING GOOGLE GEMINI (MULTIMODAL)\n",
            "========================================\n",
            "‚úÖ Google Gemini API working!\n",
            "‚è±Ô∏è Response time: 4.74 seconds\n",
            "ü§ñ Response: Gemini, developed by Google, distinguishes itself from other AI models in several key ways, although the specifics are still evolving as Google continues to refine and release different versions:\n",
            "\n",
            "* **Multimodal Capabilities:** This is arguably Gemini's most significant differentiator.  While many AI models excel in a single modality (like text generation or image recognition), Gemini is designed to seamlessly integrate and process information across multiple modalities. This means it can understand and generate text, code, images, audio, and video, and connect these different forms of information in a coherent way.  This is a significant leap beyond models that primarily focus on text or images alone.\n",
            "\n",
            "* **Advanced Reasoning and Problem-Solving:** Google emphasizes Gemini's enhanced reasoning and problem-solving abilities.  While many large language models can generate human-like text, Gemini aims to demonstrate a deeper understanding of context and the ability to tackle more complex tasks requiring logical deduction and planning.  This is often demonstrated through benchmarks and specific examples showcasing its ability to solve complex problems.\n",
            "\n",
            "* **Integration with Google's Ecosystem:** Gemini is deeply integrated with Google's vast ecosystem of products and services. This allows for seamless integration into various applications, potentially leading to more natural and intuitive user experiences across different platforms.\n",
            "\n",
            "* **Scale and Training Data:** Like other leading AI models, Gemini benefits from being trained on a massive dataset.  The sheer scale of Google's resources allows for training on a broader and potentially more diverse range of data, potentially leading to improved performance and generalization capabilities.\n",
            "\n",
            "* **Emphasis on Safety and Responsibility:** Google has publicly stated a strong commitment to building safe and responsible AI.  While the specifics of their safety protocols are not fully public, this focus is a key differentiator, particularly in an era of increasing concerns about AI bias and misuse.\n",
            "\n",
            "\n",
            "However, it's important to note that the exact differences between Gemini and other models are constantly evolving.  The field of AI is rapidly advancing, and new models are frequently released with improved capabilities.  Direct comparisons are often difficult due to the lack of consistent benchmarking across all models and the proprietary nature of some training data and architectures.  The claims made by Google about Gemini's capabilities need to be critically evaluated through independent testing and real-world applications.\n",
            "\n",
            "üí∞ Cost: $0.075 per 1M tokens\n",
            "üé® Special: Handles images, video, audio!\n",
            "\n",
            "üìã Copy-paste template for projects:\n",
            "```python\n",
            "from langchain_google_genai import ChatGoogleGenerativeAI\n",
            "llm = ChatGoogleGenerativeAI(\n",
            "    google_api_key=os.getenv('GOOGLE_API_KEY'),\n",
            "    model='gemini-1.5-flash',\n",
            "    temperature=0.3\n",
            ")\n",
            "response = llm.invoke('Your prompt here')\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Test Google Gemini API\n",
        "print(\"üé® TESTING GOOGLE GEMINI (MULTIMODAL)\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "google_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if google_key and google_key != \"your_google_api_key_here\":\n",
        "    try:\n",
        "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "        \n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            google_api_key=google_key,\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0.3\n",
        "        )\n",
        "        \n",
        "        start_time = time.time()\n",
        "        test_response = llm.invoke(\"What makes Gemini different from other AI models?\")\n",
        "        response_time = time.time() - start_time\n",
        "        \n",
        "        print(\"‚úÖ Google Gemini API working!\")\n",
        "        print(f\"‚è±Ô∏è Response time: {response_time:.2f} seconds\")\n",
        "        print(f\"ü§ñ Response: {test_response.content}\")\n",
        "        print(\"üí∞ Cost: $0.075 per 1M tokens\")\n",
        "        print(\"üé® Special: Handles images, video, audio!\")\n",
        "        \n",
        "        print(\"\\nüìã Copy-paste template for projects:\")\n",
        "        print(\"```python\")\n",
        "        print(\"from langchain_google_genai import ChatGoogleGenerativeAI\")\n",
        "        print(\"llm = ChatGoogleGenerativeAI(\")\n",
        "        print(\"    google_api_key=os.getenv('GOOGLE_API_KEY'),\")\n",
        "        print(\"    model='gemini-1.5-flash',\")\n",
        "        print(\"    temperature=0.3\")\n",
        "        print(\")\")\n",
        "        print(\"response = llm.invoke('Your prompt here')\")\n",
        "        print(\"```\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Google Gemini error: {str(e)}\")\n",
        "else:\n",
        "    print(\"‚ùå Google API key not found in .env file\")\n",
        "    print(\"üîë Get your key: https://makersuite.google.com/app/apikey\")\n",
        "    print(\"üìù Add to .env: GOOGLE_API_KEY=your_key_here\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Cell 6: Anthropic Claude (Premium: $3/1M tokens)\n",
        "\n",
        "Test Anthropic Claude API - best for reasoning and complex analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† TESTING ANTHROPIC CLAUDE (PREMIUM)\n",
            "========================================\n",
            "‚ùå Anthropic API key not found in .env file\n",
            "üîë Get your key: https://console.anthropic.com/\n",
            "üìù Add to .env: ANTHROPIC_API_KEY=your_key_here\n"
          ]
        }
      ],
      "source": [
        "# Test Anthropic Claude API\n",
        "print(\"üß† TESTING ANTHROPIC CLAUDE (PREMIUM)\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "if anthropic_key and anthropic_key != \"your_anthropic_api_key_here\":\n",
        "    try:\n",
        "        from langchain_anthropic import ChatAnthropic\n",
        "        \n",
        "        llm = ChatAnthropic(\n",
        "            anthropic_api_key=anthropic_key,\n",
        "            model=\"claude-3-5-sonnet-20241022\",\n",
        "            temperature=0.3\n",
        "        )\n",
        "        \n",
        "        start_time = time.time()\n",
        "        test_response = llm.invoke(\"Analyze the pros and cons of using AI agents in business.\")\n",
        "        response_time = time.time() - start_time\n",
        "        \n",
        "        print(\"‚úÖ Anthropic Claude API working!\")\n",
        "        print(f\"‚è±Ô∏è Response time: {response_time:.2f} seconds\")\n",
        "        print(f\"ü§ñ Response: {test_response.content[:200]}...\")\n",
        "        print(\"üí∞ Cost: $3.00 per 1M tokens\")\n",
        "        print(\"üéØ Best for: Complex reasoning, analysis, safety-critical tasks\")\n",
        "        \n",
        "        print(\"\\nüìã Copy-paste template for projects:\")\n",
        "        print(\"```python\")\n",
        "        print(\"from langchain_anthropic import ChatAnthropic\")\n",
        "        print(\"llm = ChatAnthropic(\")\n",
        "        print(\"    anthropic_api_key=os.getenv('ANTHROPIC_API_KEY'),\")\n",
        "        print(\"    model='claude-3-5-sonnet-20241022',\")\n",
        "        print(\"    temperature=0.3\")\n",
        "        print(\")\")\n",
        "        print(\"response = llm.invoke('Your prompt here')\")\n",
        "        print(\"```\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Anthropic Claude error: {str(e)}\")\n",
        "else:\n",
        "    print(\"‚ùå Anthropic API key not found in .env file\")\n",
        "    print(\"üîë Get your key: https://console.anthropic.com/\")\n",
        "    print(\"üìù Add to .env: ANTHROPIC_API_KEY=your_key_here\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Cell 7: OpenAI GPT (Standard: $2.5/1M tokens)\n",
        "\n",
        "Test OpenAI GPT API - industry standard and widely compatible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ TESTING OPENAI GPT (STANDARD)\n",
            "===================================\n",
            "‚ùå OpenAI GPT error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        }
      ],
      "source": [
        "# Test OpenAI GPT API\n",
        "print(\"ü§ñ TESTING OPENAI GPT (STANDARD)\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if openai_key and openai_key != \"your_openai_api_key_here\":\n",
        "    try:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        \n",
        "        llm = ChatOpenAI(\n",
        "            api_key=openai_key,\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0.3\n",
        "        )\n",
        "        \n",
        "        start_time = time.time()\n",
        "        test_response = llm.invoke(\"Describe the evolution of language models in 2 sentences.\")\n",
        "        response_time = time.time() - start_time\n",
        "        \n",
        "        print(\"‚úÖ OpenAI GPT API working!\")\n",
        "        print(f\"‚è±Ô∏è Response time: {response_time:.2f} seconds\")\n",
        "        print(f\"ü§ñ Response: {test_response.content}\")\n",
        "        print(\"üí∞ Cost: $0.15-2.50 per 1M tokens (model dependent)\")\n",
        "        print(\"üéØ Best for: General purpose, when specifically GPT is required\")\n",
        "        \n",
        "        print(\"\\nüìã Copy-paste template for projects:\")\n",
        "        print(\"```python\")\n",
        "        print(\"from langchain_openai import ChatOpenAI\")\n",
        "        print(\"llm = ChatOpenAI(\")\n",
        "        print(\"    api_key=os.getenv('OPENAI_API_KEY'),\")\n",
        "        print(\"    model='gpt-4o-mini',\")\n",
        "        print(\"    temperature=0.3\")\n",
        "        print(\")\")\n",
        "        print(\"response = llm.invoke('Your prompt here')\")\n",
        "        print(\"```\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenAI GPT error: {str(e)}\")\n",
        "else:\n",
        "    print(\"‚ùå OpenAI API key not found in .env file\")\n",
        "    print(\"üîë Get your key: https://platform.openai.com/api-keys\")\n",
        "    print(\"üìù Add to .env: OPENAI_API_KEY=your_key_here\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Cell 8: Setup Summary & Provider Switching Template\n",
        "\n",
        "Review your setup and get a universal template for switching providers in any project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä LLM PROVIDER SETUP SUMMARY\n",
            "===================================\n",
            "Provider             Status       Cost           \n",
            "--------------------------------------------------\n",
            "Ollama (Local)       ‚úÖ Working    FREE           \n",
            "DeepSeek             ‚úÖ Configured $0.14/1M       \n",
            "Groq                 ‚úÖ Configured $0.05/1M       \n",
            "Google Gemini        ‚úÖ Configured $0.075/1M      \n",
            "Anthropic Claude     ‚ùå No API key $3.00/1M       \n",
            "OpenAI GPT           ‚úÖ Configured $0.15-2.50/1M  \n",
            "\n",
            "üéØ Total configured: 5/6 providers\n",
            "üéâ Excellent! You have great flexibility for course projects!\n",
            "\n",
            "üîÑ UNIVERSAL PROVIDER SWITCHING TEMPLATE\n",
            "=============================================\n",
            "Copy this template to any course project:\n",
            "\n",
            "```python\n",
            "# Choose your provider (uncomment one):\n",
            "\n",
            "# FREE Local AI:\n",
            "# from langchain_ollama import OllamaLLM\n",
            "# llm = OllamaLLM(model='llama3.2', temperature=0.3)\n",
            "\n",
            "# Ultra-cheap DeepSeek:\n",
            "# from langchain_openai import ChatOpenAI\n",
            "# llm = ChatOpenAI(api_key=os.getenv('DEEPSEEK_API_KEY'),\n",
            "#                  base_url='https://api.deepseek.com',\n",
            "#                  model='deepseek-chat', temperature=0.3)\n",
            "\n",
            "# Ultra-fast Groq:\n",
            "# from groq import Groq\n",
            "# client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
            "# # Use: client.chat.completions.create(...)\n",
            "\n",
            "# Multimodal Google:\n",
            "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
            "# llm = ChatGoogleGenerativeAI(google_api_key=os.getenv('GOOGLE_API_KEY'),\n",
            "#                              model='gemini-1.5-flash', temperature=0.3)\n",
            "\n",
            "# Premium Claude:\n",
            "# from langchain_anthropic import ChatAnthropic\n",
            "# llm = ChatAnthropic(anthropic_api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
            "#                     model='claude-3-5-sonnet-20241022', temperature=0.3)\n",
            "\n",
            "# Standard OpenAI:\n",
            "# from langchain_openai import ChatOpenAI\n",
            "# llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'),\n",
            "#                  model='gpt-4o-mini', temperature=0.3)\n",
            "\n",
            "# Then use the same way:\n",
            "# response = llm.invoke('Your prompt here')\n",
            "# print(response)\n",
            "```\n",
            "\n",
            "‚úÖ You're ready to use any LLM provider in course projects!\n",
            "üéØ Just uncomment your preferred provider and start building!\n"
          ]
        }
      ],
      "source": [
        "# Setup Summary\n",
        "print(\"üìä LLM PROVIDER SETUP SUMMARY\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "providers = [\n",
        "    {\"name\": \"Ollama (Local)\", \"check\": \"http://localhost:11434/api/version\", \"cost\": \"FREE\"},\n",
        "    {\"name\": \"DeepSeek\", \"check\": os.getenv(\"DEEPSEEK_API_KEY\"), \"cost\": \"$0.14/1M\"},\n",
        "    {\"name\": \"Groq\", \"check\": os.getenv(\"GROQ_API_KEY\"), \"cost\": \"$0.05/1M\"},\n",
        "    {\"name\": \"Google Gemini\", \"check\": os.getenv(\"GOOGLE_API_KEY\"), \"cost\": \"$0.075/1M\"},\n",
        "    {\"name\": \"Anthropic Claude\", \"check\": os.getenv(\"ANTHROPIC_API_KEY\"), \"cost\": \"$3.00/1M\"},\n",
        "    {\"name\": \"OpenAI GPT\", \"check\": os.getenv(\"OPENAI_API_KEY\"), \"cost\": \"$0.15-2.50/1M\"}\n",
        "]\n",
        "\n",
        "working_count = 0\n",
        "print(f\"{'Provider':<20} {'Status':<12} {'Cost':<15}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for provider in providers:\n",
        "    if provider[\"name\"] == \"Ollama (Local)\":\n",
        "        try:\n",
        "            response = requests.get(provider[\"check\"], timeout=2)\n",
        "            status = \"‚úÖ Working\" if response.status_code == 200 else \"‚ùå Not running\"\n",
        "            if \"Working\" in status:\n",
        "                working_count += 1\n",
        "        except:\n",
        "            status = \"‚ùå Not installed\"\n",
        "    else:\n",
        "        key = provider[\"check\"]\n",
        "        if key and key not in [\"your_deepseek_api_key_here\", \"your_groq_api_key_here\", \n",
        "                               \"your_google_api_key_here\", \"your_anthropic_api_key_here\", \n",
        "                               \"your_openai_api_key_here\"]:\n",
        "            status = \"‚úÖ Configured\"\n",
        "            working_count += 1\n",
        "        else:\n",
        "            status = \"‚ùå No API key\"\n",
        "    \n",
        "    print(f\"{provider['name']:<20} {status:<12} {provider['cost']:<15}\")\n",
        "\n",
        "print(f\"\\nüéØ Total configured: {working_count}/6 providers\")\n",
        "\n",
        "if working_count >= 3:\n",
        "    print(\"üéâ Excellent! You have great flexibility for course projects!\")\n",
        "elif working_count >= 1:\n",
        "    print(\"üëç Good start! You can begin course projects.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Configure at least one provider to start.\")\n",
        "\n",
        "print(\"\\nüîÑ UNIVERSAL PROVIDER SWITCHING TEMPLATE\")\n",
        "print(\"=\" * 45)\n",
        "print(\"Copy this template to any course project:\")\n",
        "print(\"\\n```python\")\n",
        "print(\"# Choose your provider (uncomment one):\")\n",
        "print(\"\\n# FREE Local AI:\")\n",
        "print(\"# from langchain_ollama import OllamaLLM\")\n",
        "print(\"# llm = OllamaLLM(model='llama3.2', temperature=0.3)\")\n",
        "print(\"\\n# Ultra-cheap DeepSeek:\")\n",
        "print(\"# from langchain_openai import ChatOpenAI\")\n",
        "print(\"# llm = ChatOpenAI(api_key=os.getenv('DEEPSEEK_API_KEY'),\")\n",
        "print(\"#                  base_url='https://api.deepseek.com',\")\n",
        "print(\"#                  model='deepseek-chat', temperature=0.3)\")\n",
        "print(\"\\n# Ultra-fast Groq:\")\n",
        "print(\"# from groq import Groq\")\n",
        "print(\"# client = Groq(api_key=os.getenv('GROQ_API_KEY'))\")\n",
        "print(\"# # Use: client.chat.completions.create(...)\")\n",
        "print(\"\\n# Multimodal Google:\")\n",
        "print(\"# from langchain_google_genai import ChatGoogleGenerativeAI\")\n",
        "print(\"# llm = ChatGoogleGenerativeAI(google_api_key=os.getenv('GOOGLE_API_KEY'),\")\n",
        "print(\"#                              model='gemini-1.5-flash', temperature=0.3)\")\n",
        "print(\"\\n# Premium Claude:\")\n",
        "print(\"# from langchain_anthropic import ChatAnthropic\")\n",
        "print(\"# llm = ChatAnthropic(anthropic_api_key=os.getenv('ANTHROPIC_API_KEY'),\")\n",
        "print(\"#                     model='claude-3-5-sonnet-20241022', temperature=0.3)\")\n",
        "print(\"\\n# Standard OpenAI:\")\n",
        "print(\"# from langchain_openai import ChatOpenAI\")\n",
        "print(\"# llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'),\")\n",
        "print(\"#                  model='gpt-4o-mini', temperature=0.3)\")\n",
        "print(\"\\n# Then use the same way:\")\n",
        "print(\"# response = llm.invoke('Your prompt here')\")\n",
        "print(\"# print(response)\")\n",
        "print(\"```\")\n",
        "\n",
        "print(\"\\n‚úÖ You're ready to use any LLM provider in course projects!\")\n",
        "print(\"üéØ Just uncomment your preferred provider and start building!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
